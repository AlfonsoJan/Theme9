---
output:
  pdf_document:
    pandoc_args: --highlight=breezedark
---

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment=NA)
```


<!-- Copyright (c) 2022 John Busker. -->
<!-- Licensed under GPLv3. See gpl.md -->


# Machine Learning Research log

## Can machine learning predict protein localization based on their aminoacid sequences

\vspace*{\fill}
\begin{flushright}
John Busker\\
352905\\
Bio-Informatics\\
Dave Langers, and Bart Barnard\\
28-10-2022
\end{flushright}

\newpage

# Table of Contents

1. [R Environment](#r-environment) \dotfill 3  

2. [EDA](#eda) \dotfill 4  
  2.1 [About The Data](#about-the-data) \dotfill 4  
  2.2 [Codebook](#codebook) \dotfill 5  
  2.3 [Loading The Data](#loading-the-data) \dotfill 6  
  2.4 [Cleaning The Data](#cleaning-the-data) \dotfill 6  
  2.5 [Data Transformation](#data-transformation) \dotfill 9  
  2.6 [Univariate Analysis](#univariate-analysis) \dotfill 9  
  2.7 [Bivariate Analysis](#bivariate-analysis) \dotfill 12   
  2.8 [Class Labels](#class-labels) \dotfill 15  
  2.9 [Multivariate Analysis](#multivariate-analysis) \dotfill 18  
  
3. [MIL Algorithms](#mil-algorithms) \dotfill 20  
  3.1 [Criteria For MIL Algorithms](#criteria-for-mil-algorithms) \dotfill 20  

\newpage

# R Environment

Multiple libraries have been uses for data analysis and further processes. The libraries used are:

```{r setup, message=FALSE}
library(tidyr)
library(kableExtra)
library(ggplot2)
library(gridExtra)
library(cowplot)
library(ggpubr)
library(pander)
library(ggbiplot)
library(reshape2)
library(ggfortify)
library(ggalt)
library(ggridges)
library(cluster)
```

\newpage 

# EDA

## About The Data

The data has been retrieved from [uci.](https://archive.ics.uci.edu/ml/datasets/Yeast) The dataset consists of 1484 yeast sequences from SWISS-PROT using the annotations from YPD. In this section, we are going to talk about the results we found from the dataset. Eight features were used in classification: the presence or absence of an HDEL pattern as a signal for retention in the endoplasmic reticulum lumen (erl); The results of discriminant analysis on the amino acid content of vacuolar and extracellular proteins (vac); the result of discriminant analysis on the amino acid composition of the 20-residue N-terminal region of mitochondrial and non-mitochondrial proteins (mit); the presence or absence of nuclear localization consensus patterns combined with a term reflecting the frequency of basic residues (nuc); and some combination of the presence of a short sequence motif and the result of discriminant analysis of the amino acid composition of the protein sequence (pox).

Attributes information:  

* **Sequence Name**: Accession number for the SWISS-PROT database.  
* **mcg**: McGeoch's method for signal sequence recognition.  
* **gvh**: von Heijne's method for signal sequence recognition.  
* **alm**: Score of the ALOM membrane spanning region prediction program.  
* **mit**: Score of discriminant analysis of the amino acid content of the N-terminal region (20 residues long) of mitochondrial and non-mitochondrial proteins.  
* **erl**: Presence of "HDEL" substring (thought to act as a signal for retention in the endoplasmic reticulum lumen). Binary attribute.  
* **pox**: Peroxisomal targeting signal in the C-terminus.  
* **vac**: Score of discriminant analysis of the amino acid content of vacuolar and extracellular proteins.  
* **nuc**: Score of discriminant analysis of nuclear localization signals of nuclear and non-nuclear proteins.  
* **Class Distribution**: The class is the localization site. Consisting of (abbreviation (full name) the amount):

--- ---------------------------------------------- ---
CYT (cytosolic or cytoskeletal)                    463  
NUC (nuclear)                                      429  
MIT (mitochondrial)                                244  
ME3 (membrane protein, no N-terminal signal)       163  
ME2 (membrane protein, uncleaved signal)            51  
ME1 (membrane protein, cleaved signal)              44  
EXC (extracellular)                                 37  
VAC (vacuolar)                                      30  
POX (peroxisomal)                                   20  
ERL (endoplasmic reticulum lumen)                    5  
--- ---------------------------------------------- --

Yeast proteins were classified into ten classes: **cytoplasmic:** cytoskeletal (CYT); nuc]ear (NUC); vacuolar (VAC); mitochondrial (MIT); isomal (POX); **extracellular:** including those localized against the cell wall (EXC); proteins localized to the lumen of the endoplasmic reticulum (ERL); membrane proteins with a cleaved signal (ME1); membrane proteins with an uncleared signal (ME2); and membrane proteins with no N-terminal sign (ME3), where ME1, ME2,and ME3 proteins may be localized to the plasma membrane, the endoplasmic reticulum membrane, or the membrane of a golgi body.

## Codebook

Since there is not a codebook. A own codebook has been created

```{r}
# All the attributes name
attr_names <- c("seq.name", "mcg", "gvh", "alm", "mit", "erl",
                "pox", "vac","nuc", "loc.site" )

data_types <- c("str", "float", "float", "float", "float", "float",
                "bool", "float", "float", "factor")

# The description of the labels
data_labels <- c("Accession number for the SWISS-PROT database",
                 "McGeoch's method for signal sequence recognition",
                 "von Heijne's method for signal sequence recognition",
                 "Score of the ALOM membrane spanning region prediction program",
                 "Score of discriminant analysis of the amino 
                 acid content of the N-terminal region",
                 "Presence of 'HDEL' substring",
                 "Peroxisomal targeting signal in the C-terminus",
                 "Score of discriminant analysis of the amino acid content 
                 of vacuolar and extracellular proteins",
                 "Score of discriminant analysis of nuclear 
                 localization signals of nuclear and non-nuclear proteins",
                 "The class is the localization site")

codebook <- data.frame(Name=attr_names, 
                       Fullname=data_labels, 
                       Datatypes=data_types)

pander(codebook)
```

Here is the codebook. With the attributes abbreviation, explanation and data types. As you can see there are many float datatypes.

## Loading The Data

Now we need to load in the data. And change the column names to the attributes abbreviation, since these are non-existent. Let's give all the columns a name. And let's take a quick look at the data.

```{r}
# Read the file in ass a tibble
data <- as_tibble(read.table("Resources/yeast.data", sep = ""))
colnames(data) <- attr_names
head(data)
```

The yeast data set contains scores per cellular localization sites. The last column is the sequence's localization site. There are ten different possibilities for this. 

## Cleaning The Data

We can drop the first columns since it is not necessary. Since the sequence names contribute nothing to create a prediction model.

```{r}
# Drop the first column
data <- data[, -1]
head(data)
```

The first column has been succesfully dropped from the data.

Are there any missing values? Let's take a quick look.

```{r}
# Count all the NA values
sum(is.na(data))
```

There are not any missing values. So nothing needs to be changed for this.

Now we need to take a clearer look at the data set using `str()`.

```{r}
str(data)
```

As you can see, the last column, loc.site, consists of characters, but this one needs to be converted to factors for clarity and complexity.

```{r}
# Transform the last column to a factor
data$loc.site <- as.factor(data$loc.site)
str(data$loc.site)
```

As you can see the data type of the column loc.site has been succesfully changed to factors.

There are alot of rows. Let's visualize the amount of each classification.

```{r, fig.cap="A waffle chart of the categorical composition"}
# A grid to fill in 28x53
df <- expand.grid(y = 1:28, x = 1:53)
# Sort the table
categ_table <- sort(table(data$loc.site), decreasing = T)
df$category <- factor(rep(names(categ_table), categ_table))  

ggplot(df, aes(x = x, y = y, fill = category)) + 
        geom_tile(color = "black", size = 0.5) +
        scale_x_continuous(expand = c(0, 0)) +
        scale_y_continuous(expand = c(0, 0), trans = 'reverse') +
        scale_fill_brewer(palette = "Set3") + 
        labs(title="Waffle Chart", subtitle="'Class' of localization") + 
        xlab(NULL) + ylab(NULL)
```

\newpage

There are alot of CYT and NUC localization. This probably means that CYT has a greater variation in localisation than ERL.

## Data Transformation

The ERL variable also should be changed. As you can see, it is now a num data type but I needs to be a bool/binary datatype. Let's take a closer look at th ERL column.

```{r}
summary(data$erl)
```

This looks weird because the value should be 0 or 1.

There are alot of 0.5 values. But should not exist in this column. The variable need to be a bool, or in other words it must be a 0 or a 1. So all the 0.5 values needs to be changed to a 0.

```{r}
table(data[, "erl"])
```

Before the data transformation there are 1470 counts of the value 0.5 and 14 of 1.

```{r}
# Change every 0.5 value to a 0
data$erl[data$erl == 0.5] <- 0
table(data[,"erl"])
```

The data has been succesfully transformed. Now the datatype has to be changed to a bool.

```{r}
# Change the datatype to a logical
data$erl <- as.logical(data$erl)
str(data$erl)
```

Now every column has the right datatype.

## Univariate Analysis

Let's take a quick look at the data using `summary()`. We can drop column 5 and 9 for this. Column 5, ERL, is a logical datatype and column 9, loc.site, is a string datatype.

```{r}
summary(data[, -c(5,9)])
```

As you can see all the datapoints are between 0 and 1. Se the data already has been transformed with a min-max normalization.

Let's visualise this with ggplot. Using jitterpoints and a violing plot.

```{r, fig.cap="Boxplot comparing basic statistic for all columns", fig.height=12, fig.width=6}
p1 <- ggplot(data, mapping = aes(x = "", y = mcg)) + geom_violin(alpha=0.2) + 
  geom_jitter(width = 0.2, alpha = 0.25, height = 0, color = "red") + xlab(NULL)
p2 <- ggplot(data, mapping = aes(x = "", y = gvh)) + geom_violin(alpha=0.2) + 
  geom_jitter(width = 0.2, alpha = 0.25, height = 0, color = "blue") + xlab(NULL)
p3 <- ggplot(data, mapping = aes(x = "", y = alm)) + geom_violin(alpha=0.2) + 
  geom_jitter(width = 0.2,alpha = 0.25, height = 0, color = "purple") + xlab(NULL)
p4 <- ggplot(data, mapping = aes(x = "", y = mit)) + geom_violin(alpha=0.2) + 
  geom_jitter(width = 0.2, alpha = 0.25, height = 0, color = "brown") + xlab(NULL)
p5 <- ggplot(data, mapping = aes(x = "", y = pox)) + geom_violin(alpha=0.2) + 
  geom_jitter(width = 0.2,alpha = 0.25, height = 0, color = "orange") + xlab(NULL)
p6 <- ggplot(data, mapping = aes(x = "", y = vac)) + geom_violin(alpha=0.2) + 
  geom_jitter(width = 0.2, alpha = 0.25, height = 0, color = "green") + xlab(NULL)
p7 <- ggplot(data, mapping = aes(x = "", y = nuc)) + geom_violin(alpha=0.2) + 
  geom_jitter(width = 0.2,alpha = 0.25, height = 0, color = "orange") + xlab(NULL)

plot <- ggarrange(p1, p2, p3, p4, p5, p6, p7, nrow = 4, ncol = 2)
annotate_figure(plot, top = text_grob("Boxplots", face = "bold", size = 14))
```

\newpage

Mcg and gvh are normally distributed. Alm and mit are skewed but nothing crazy to worry about.
Vac and nuc are really skewed however, not yet bad enough to worry about it. Pox is looks weird let's take a detailed look at pox.

```{r}
table(data[, "pox"])
```

As you can see almost all the numbers are 0. This does not mean that we just discord this column. Maybe it is very significant if the number is not 0.

```{r}
# Subset the data where pox is larger than 0
subset(data, pox > 0, select=loc.site)
```

If the number is non-zero then the probability is very high that the protein is localised in peroxisomal region.

## Bivariate Analysis

With a heatmap, you can easily see where there are correlations between variables. First let's create a correlation matrix

```{r, warning=FALSE}
# Create a cor matrix
cor_matrix <- cor(data[, -c(5,9)])
cor_matrix <- as_tibble(cor_matrix)
# Add a column with the varibale names
cor_matrix <- cor_matrix %>% mutate(varnames = all_of(attr_names)[-c(1,6,10)])
kbl(cor_matrix, booktabs = T, align = "c") %>%
  kable_styling(full_width = T)
```

The calculated correlation matrix. This needs to be tranformed to a long matrix.

```{r, warning=FALSE}
# Create a long matrix
cols <- all_of(attr_names)[-c(1,6,10)]
cor_matrix_long <- pivot_longer(data = cor_matrix, 
                                 cols = cols,
                                 names_to = "variable", values_to = "cor")
pander(head(cor_matrix_long))
```

The long calculated correlation matrix. Let's visualise this using a heatmap.

```{r, fig.cap="A heatmap pairwise correlation of selected numeric variables"}

ggplot(data = cor_matrix_long, aes(x=varnames, y=variable, fill=cor)) + 
    geom_tile() + 
    labs(x=NULL, y=NULL, title="Heatmap Correlation") + 
    scale_fill_gradient(high = "purple", low = "white" )
```

\newpage

As seen from the heatmap there is some correlation between mcg and gvh. Let's visualise this.

```{r, warning=FALSE, fig.cap="Scatterplot with trendline with the dependent variables"}
ggplot(data, aes(x=mcg, y=gvh, color=loc.site)) +
        labs(x="MCG", y="GVH") +
    geom_jitter(mapping = aes(color=loc.site), 
                na.rm=T, width=0.2, height=0.2, 
                alpha=0.5, shape=16, size=0.8) + 
    ylim(0,1) + labs(title="Scatterplot and trendline") +
    geom_smooth(formula = y ~ x, method = "loess")
```

Every variable goes with a slow upward trend except erl this one has a weird curve.

## Class Labels

Now we need to look at how the data correlates with the classes. The heigth of the peak doesn’t matter much, shifted peaks do.

```{r,fig.height=6, fig.width=8, fig.cap="Density plots shows class distinction"}
p1 <- ggplot(data, aes(x=mcg)) + geom_density(aes(color=loc.site))
p2 <- ggplot(data, aes(x=gvh)) + geom_density(aes(color=loc.site))
p3 <- ggplot(data, aes(x=alm)) + geom_density(aes(color=loc.site))
p4 <- ggplot(data, aes(x=mit)) + geom_density(aes(color=loc.site))
p5 <- ggplot(data, aes(x=pox)) + geom_density(aes(color=loc.site))
p6 <- ggplot(data, aes(x=vac)) + geom_density(aes(color=loc.site))
p7 <- ggplot(data, aes(x=nuc)) + geom_density(aes(color=loc.site))
plot <- ggarrange(p1, p2, p3, p4, p5, p6, p7, ncol=4, nrow=2, 
          common.legend=TRUE, legend="right")
annotate_figure(plot, top = text_grob("Density plots", face = "bold", size = 14))
```

At pox, vac and nuc, you don't see shifted peaks. At mcg and gvh you really see shifted peaks this shows a distribution of the different classes. There are small shifted peaks at alm en mit, this shows that there is difference but not so much.
Let's look at mcg and gvh using ridge lines plot.

```{r, warning=FALSE, message=FALSE, fig.cap="Ridge line plot between mcg and gvh"}
p1 <- ggplot(data, aes(x=mcg, y=loc.site, fill = loc.site)) + 
  geom_density_ridges2(rel_min_height = 0.005) + theme_minimal() + 
  coord_cartesian(clip = "off")
p2 <- ggplot(data, aes(x=gvh, y=loc.site, fill = loc.site)) + 
  geom_density_ridges2(rel_min_height = 0.005) + theme_minimal() + 
  coord_cartesian(clip = "off")
plot <- ggarrange(p1, p2, common.legend = TRUE)
annotate_figure(plot, top = text_grob("Ridge line plot", face = "bold", size = 14))
```

\newpage

Now you can really see that the peaks are shifted.

We need to test the data to see if there is significant difference between it. Using a 1-way ANOVA test.

```{r}
# Perform a one way anova test
res.aov <- summary(aov(mcg ~ loc.site, data = data))
res.aov[[1]]$`Pr(>F)`[1]
```

The P-value is < 0.05. So there is a significant difference.

## Multivariate analysis

One way to see if groups are clustered is to MDS plot the groups and calculated the distance matrix.

```{r}
# Create a matrix
matrix <- with(data, rbind(mcg, gvh, alm, mit, pox, vac, nuc))
(distmat <- dist(matrix))
```

Here is the distance matrix.

```{r, fig.cap="Classical (Metric) Multidimensional Scaling", fig.width=4, fig.height=2.5}
autoplot(cmdscale(distmat, eig = TRUE), shape = FALSE, label = TRUE, label.size = 4)
```

As seen from above mcg and gvh are the clustered group.

A other plot to show is using PCA.

\newpage

```{r, fig.cap="PCA plot showing the groups"}
df <- data[-c(5,9)]

pca_res <- prcomp(df, scale. = TRUE)

autoplot(pca_res, data = data, colour = 'loc.site', loadings = TRUE, loadings.label = TRUE)
```

As earlier shown mcg and gvh are grouped together.

\newpage

# MIL Algorithms

First we need to investigate which standard ML algorithms is the best to use for this dataset using Weka.

Weka accepts .arff files. You can convert a .csv to a .arff file in Weka. So we need to export the clean dataset to a .csv file.

```{r, eval=FALSE}
write.table(data,"data.csv", quote = FALSE, sep = ",", row.names=FALSE)
```

Open the .csv in Weka under tools -> ArffViewer. And saved the file as .arff. The first 15 lines of the .arff file needs to look like this.

```{r}
cat(readLines("Resources/data.arff", n = 15), sep = "\n")
```

## Criteria For MIL Algorithms

An accurate algorithm is needed that accurately predicts the dataset. It needs a high percentage of correct classifications on the dataset. An algorithm that is fast. What if there is a dataset of 1 million observations. And it should also not contain too many false negatives and false positives.

Research has been done on which algorithm is the best. From every representatives of all classifier categories. And ZeroR and OneR for a baseline performance. Eg: Decision Trees (C4.5, J48, RandomForest), Nearest Neighbor (IBk), SVM (SMO), NaiveBayes (Naïve Bayes) and Linear Logistic (SimpleLogistic). And carried out classifications 10-fold cross validation.

The experimenter can save the results in a csv file.

```{r}
weka.result <- read.table("Resources/Result.csv", header = TRUE, sep = ",")
# Remove weka.classifiers.* from classifier name
for (i in 1:length(weka.result$Key_Scheme)) {
  name <- weka.result$Key_Scheme[i]
  tweaked.name <- strsplit(name, split = ".", fixed = TRUE)[[1]][4]
  weka.result$Key_Scheme[i] <- tweaked.name
}
```

Now the results of the csv can been plotted with ease of use. Like the ROC and precision.

```{r, fig.cap="Area under the curve for different classifiers"}
# Create a dataframe of the mean of the ROC of every classifier
res <- aggregate(Area_under_ROC ~ Key_Scheme, data = weka.result, mean)

ggplot(data=res, aes(x=Key_Scheme, y=Area_under_ROC, fill=Key_Scheme)) + 
  geom_bar(stat="identity") + 
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) + 
  labs(x="Classifier", y="Area under curve (ROC)", 
       title="Area under the curve for different classifiers") + 
  geom_hline(aes(yintercept=min(Area_under_ROC)), linetype = 'dashed') + 
  geom_hline(aes(yintercept=max(Area_under_ROC)), linetype = 'dashed') + 
  geom_text(aes(label=round(Area_under_ROC, digits=2)), vjust=1.5, size=3.5)
```

RandomForest has the highest area under the curve, while OneR has the lowest with 0.49 and ZeroR with a close second last with 0.5.

\newpage

```{r, fig.cap="Average precisionfor different classifiers"}
res <- aggregate(IR_precision ~ Key_Scheme, data = weka.result, mean)

ggplot(data=res, aes(x=Key_Scheme, y=IR_precision, fill=Key_Scheme)) + 
  geom_bar(stat="identity") + 
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) + 
  labs(x="Classifier", y="Precision", 
       title="Average precision for different classifiers") + 
  geom_hline(aes(yintercept=min(IR_precision)), linetype = 'dashed') + 
  geom_hline(aes(yintercept=max(IR_precision)), linetype = 'dashed') + 
  geom_text(aes(label=round(IR_precision, digits=2)), vjust=1.5, size=3.5)
```

RandomForest also has the highest average precision with 0.66. And OneR has the lowest with 0.04. ZeroR is not in this plots because it only consist of NaN.

Several algorithms cannot make a prediction for the classification VAC (0.000) and for some they get an error (?). 1 algorithm IBk has a very low prediction for VAC. All the algorithm above in the figure except for ZeroR, OneR, SMO and SimpleLogistic were used in the Weka Experimenter to compare.

![Ranking of chosen algorithms]("Resources/images/RankingAlgo.png")

This shows the ranking of the algorithms by the number of times a given algorithm beat the other algorithms. And RandomForest beat all 4 algorithms.

![Ranking of chosen algorithms by SD]("Resources/images/WekaSD.png")

RandomForest has a 'v' next to their result. This means that the difference in the accuracy for these algorithms compared to all the other algorithms is significant.

After testing multiple algorithms in Weka, the algorithm RandomForest was chosen. It has the highest average precision of 0.611. And this was one of the few algorithms where the precision for VAC was not a '?'.

But for the ratings, no algorithm could make a prediction except 1 for VAC, however, it was also very low. So we looked at whether there is siginificant difference if we remove this classifier.

![Removed class difference]("Resources/images/WekaDataEdit.png")

And by the looks of it, there is no significant difference. So VAC is not removed from the dataset.

RandomForest has seven attributes. Below is a table with each attribute and its meaning.

```{r, echo=FALSE}
attr <- c("P", "I", "num-slots", "K", "M", "V", "S")
attr.descr <- c("Size of each bag, as a percentage of the training set size.", 
                "Number of iterations.", "Number of execution slots.", 
                "Number of attributes to randomly investigate.", 
                "Number of attributes to randomly investigate.", 
                "Set minimum numeric class variance proportion.", 
                "Seed for random number generator.")
pander(data.frame(Attribute=attr, Description=attr.descr))
```

Weka will find the optimal parameters for a classifier. With CVParameterSelection in Weka were the best parameters found.

```{r, echo=FALSE}
old.val <- c(100, 100, 1, 0, 1, 0.001, 1)
new.val <- c(50, 1000, 0, 1, 1, 0.001, 1)
pander(data.frame(Attribute=attr, Old.Values=old.val, New.Values=new.val))
```

This table shows the parameters with their default settings and their new. P, I, num-slots and K were different than the default settings. The build will take more time because it does 10 times more I(terations). 

![Iterations ranked]("Resources/images/WekaIterRank.png")

Number 1 in the figure above is with 1000 I(terations) and number 2 is 100 I(terations). And there is not a significant difference. So for performance, the value for I will be 100.

Weka Meta learners were used to get a better idea of an optimal classifier.

```{r, echo=FALSE}
tbl <- read.table("Resources/MetaResults.txt", header = TRUE)
colnames(tbl) <- c("Classifier", "TP Rate", "FP Rate", "Precision", "ROC Area", " ")
pander(tbl)
```

As it can be seen in this table, RandomForest with tweaked parameters is the best. This one has no '?' at VAC and precision.